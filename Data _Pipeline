# src/data_pipeline/validators.py
"""
Data quality validation and checks
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from datetime import datetime
from src.config.logging_config import logger
import great_expectations as ge


class DataValidator:
    """Validate data quality and integrity"""
    
    def __init__(self):
        self.validation_results = []
    
    def validate_schema(self, df: pd.DataFrame, expected_schema: Dict) -> Tuple[bool, List[str]]:
        """
        Validate DataFrame schema matches expected structure
        
        Args:
            df: Input DataFrame
            expected_schema: Dict of {column_name: expected_dtype}
            
        Returns:
            (is_valid, list_of_errors)
        """
        errors = []
        
        # Check missing columns
        missing_cols = set(expected_schema.keys()) - set(df.columns)
        if missing_cols:
            errors.append(f"Missing columns: {missing_cols}")
        
        # Check extra columns
        extra_cols = set(df.columns) - set(expected_schema.keys())
        if extra_cols:
            logger.warning(f"Extra columns found: {extra_cols}")
        
        # Check data types
        for col, expected_dtype in expected_schema.items():
            if col in df.columns:
                if not pd.api.types.is_dtype_equal(df[col].dtype, expected_dtype):
                    errors.append(f"Column {col} has dtype {df[col].dtype}, expected {expected_dtype}")
        
        is_valid = len(errors) == 0
        if not is_valid:
            logger.error(f"Schema validation failed: {errors}")
        
        return is_valid, errors
    
    def check_missing_values(self, df: pd.DataFrame, threshold: float = 0.5) -> Dict:
        """
        Check for missing values exceeding threshold
        
        Args:
            df: Input DataFrame
            threshold: Maximum allowed proportion of missing values
            
        Returns:
            Dict with missing value statistics
        """
        missing_stats = {}
        
        for col in df.columns:
            missing_pct = df[col].isnull().sum() / len(df)
            if missing_pct > threshold:
                missing_stats[col] = missing_pct
                logger.warning(f"Column {col} has {missing_pct:.2%} missing values")
        
        return missing_stats
    
    def detect_outliers(self, df: pd.DataFrame, columns: List[str], method: str = 'iqr') -> Dict:
        """
        Detect outliers in numerical columns
        
        Args:
            df: Input DataFrame
            columns: List of columns to check
            method: 'iqr' or 'zscore'
            
        Returns:
            Dict with outlier counts per column
        """
        outlier_counts = {}
        
        for col in columns:
            if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):
                continue
            
            if method == 'iqr':
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
            elif method == 'zscore':
                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                outliers = (z_scores > 3).sum()
            
            outlier_counts[col] = outliers
            if outliers > 0:
                logger.info(f"Column {col} has {outliers} outliers ({outliers/len(df):.2%})")
        
        return outlier_counts
    
    def validate_date_range(self, df: pd.DataFrame, date_col: str, 
                           min_date: datetime = None, max_date: datetime = None) -> bool:
        """Validate dates fall within expected range"""
        
        if date_col not in df.columns:
            logger.error(f"Date column {date_col} not found")
            return False
        
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        
        invalid_dates = df[date_col].isnull().sum()
        if invalid_dates > 0:
            logger.warning(f"{invalid_dates} invalid dates in {date_col}")
        
        if min_date and (df[date_col] < min_date).any():
            logger.error(f"Dates before {min_date} found in {date_col}")
            return False
        
        if max_date and (df[date_col] > max_date).any():
            logger.error(f"Dates after {max_date} found in {date_col}")
            return False
        
        return True
    
    def check_data_freshness(self, df: pd.DataFrame, timestamp_col: str, 
                            max_age_hours: int = 24) -> bool:
        """Check if data is recent enough"""
        
        latest_timestamp = pd.to_datetime(df[timestamp_col]).max()
        age_hours = (datetime.now() - latest_timestamp).total_seconds() / 3600
        
        if age_hours > max_age_hours:
            logger.warning(f"Data is {age_hours:.1f} hours old (threshold: {max_age_hours})")
            return False
        
        return True


# src/data_pipeline/anonymizer.py
"""
Data anonymization for PII protection
"""
import hashlib
import pandas as pd
from typing import List
from src.config.logging_config import logger


class DataAnonymizer:
    """Anonymize personally identifiable information"""
    
    def __init__(self, salt: str = "kenya-energy-2025"):
        self.salt = salt
    
    def hash_column(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """
        Hash sensitive column values
        
        Args:
            df: Input DataFrame
            column: Column name to hash
            
        Returns:
            DataFrame with hashed column
        """
        if column not in df.columns:
            logger.warning(f"Column {column} not found for hashing")
            return df
        
        df = df.copy()
        df[column] = df[column].apply(
            lambda x: hashlib.sha256(f"{x}{self.salt}".encode()).hexdigest() if pd.notna(x) else None
        )
        
        logger.info(f"Hashed column: {column}")
        return df
    
    def mask_phone_numbers(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """Mask phone numbers keeping only last 4 digits"""
        
        df = df.copy()
        df[column] = df[column].astype(str).str[-4:].str.pad(width=10, side='left', fillchar='*')
        
        logger.info(f"Masked phone numbers in column: {column}")
        return df
    
    def generalize_location(self, df: pd.DataFrame, lat_col: str, lon_col: str, 
                           precision: int = 2) -> pd.DataFrame:
        """
        Reduce location precision to protect privacy
        
        Args:
            df: Input DataFrame
            lat_col: Latitude column
            lon_col: Longitude column
            precision: Decimal places to keep
        """
        df = df.copy()
        df[lat_col] = df[lat_col].round(precision)
        df[lon_col] = df[lon_col].round(precision)
        
        logger.info(f"Generalized location data to {precision} decimal places")
        return df
    
    def anonymize_dataset(self, df: pd.DataFrame, pii_columns: Dict[str, str]) -> pd.DataFrame:
        """
        Apply anonymization to multiple columns
        
        Args:
            df: Input DataFrame
            pii_columns: Dict of {column_name: anonymization_method}
                        Methods: 'hash', 'mask', 'remove'
        """
        df = df.copy()
        
        for col, method in pii_columns.items():
            if method == 'hash':
                df = self.hash_column(df, col)
            elif method == 'mask':
                df = self.mask_phone_numbers(df, col)
            elif method == 'remove':
                df = df.drop(columns=[col])
                logger.info(f"Removed PII column: {col}")
        
        return df
