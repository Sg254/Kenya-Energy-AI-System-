# tests/unit/test_validators.py
"""
Unit tests for data validation
"""
import pytest
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from src.data_pipeline.validators import DataValidator


class TestDataValidator:
    """Test suite for DataValidator class"""
    
    @pytest.fixture
    def validator(self):
        """Create validator instance"""
        return DataValidator()
    
    @pytest.fixture
    def sample_data(self):
        """Create sample DataFrame for testing"""
        return pd.DataFrame({
            'customer_id': ['C001', 'C002', 'C003', 'C004'],
            'payment_amount': [100.0, 150.0, 200.0, None],
            'timestamp': pd.date_range('2025-01-01', periods=4, freq='D'),
            'usage_kwh': [10.5, 12.3, 8.7, 15.2]
        })
    
    def test_validate_schema_success(self, validator, sample_data):
        """Test schema validation with correct schema"""
        expected_schema = {
            'customer_id': 'object',
            'payment_amount': 'float64',
            'timestamp': 'datetime64[ns]',
            'usage_kwh': 'float64'
        }
        
        is_valid, errors = validator.validate_schema(sample_data, expected_schema)
        assert is_valid
        assert len(errors) == 0
    
    def test_validate_schema_missing_column(self, validator, sample_data):
        """Test schema validation with missing column"""
        expected_schema = {
            'customer_id': 'object',
            'payment_amount': 'float64',
            'missing_column': 'int64'
        }
        
        is_valid, errors = validator.validate_schema(sample_data, expected_schema)
        assert not is_valid
        assert 'Missing columns' in errors[0]
    
    def test_check_missing_values(self, validator, sample_data):
        """Test missing value detection"""
        missing_stats = validator.check_missing_values(sample_data, threshold=0.2)
        
        assert 'payment_amount' in missing_stats
        assert missing_stats['payment_amount'] == 0.25
    
    def test_detect_outliers_iqr(self, validator, sample_data):
        """Test outlier detection using IQR method"""
        outlier_counts = validator.detect_outliers(sample_data, ['usage_kwh'], method='iqr')
        
        assert 'usage_kwh' in outlier_counts
        assert isinstance(outlier_counts['usage_kwh'], (int, np.integer))
    
    def test_validate_date_range(self, validator, sample_data):
        """Test date range validation"""
        is_valid = validator.validate_date_range(
            sample_data,
            'timestamp',
            min_date=datetime(2025, 1, 1),
            max_date=datetime(2025, 12, 31)
        )
        
        assert is_valid
    
    def test_check_data_freshness_fresh(self, validator):
        """Test freshness check with recent data"""
        df = pd.DataFrame({
            'timestamp': [datetime.now() - timedelta(hours=1)]
        })
        
        is_fresh = validator.check_data_freshness(df, 'timestamp', max_age_hours=24)
        assert is_fresh
    
    def test_check_data_freshness_stale(self, validator):
        """Test freshness check with old data"""
        df = pd.DataFrame({
            'timestamp': [datetime.now() - timedelta(hours=48)]
        })
        
        is_fresh = validator.check_data_freshness(df, 'timestamp', max_age_hours=24)
        assert not is_fresh


# tests/unit/test_anonymizer.py
"""
Unit tests for data anonymization
"""
import pytest
import pandas as pd
from src.data_pipeline.anonymizer import DataAnonymizer


class TestDataAnonymizer:
    """Test suite for DataAnonymizer"""
    
    @pytest.fixture
    def anonymizer(self):
        return DataAnonymizer(salt="test_salt")
    
    @pytest.fixture
    def sensitive_data(self):
        return pd.DataFrame({
            'customer_id': ['C001', 'C002', 'C003'],
            'phone_number': ['0712345678', '0723456789', '0734567890'],
            'email': ['user1@example.com', 'user2@example.com', 'user3@example.com'],
            'latitude': [-1.286389, -1.292066, -1.303569],
            'longitude': [36.817223, 36.821945, 36.825906]
        })
    
    def test_hash_column(self, anonymizer, sensitive_data):
        """Test hashing of sensitive columns"""
        result = anonymizer.hash_column(sensitive_data.copy(), 'customer_id')
        
        # Check that values are hashed (64 character hex strings)
        assert all(len(str(x)) == 64 for x in result['customer_id'])
        
        # Check that hashing is consistent
        result2 = anonymizer.hash_column(sensitive_data.copy(), 'customer_id')
        assert result['customer_id'].equals(result2['customer_id'])
    
    def test_mask_phone_numbers(self, anonymizer, sensitive_data):
        """Test phone number masking"""
        result = anonymizer.mask_phone_numbers(sensitive_data.copy(), 'phone_number')
        
        # Check that only last 4 digits are visible
        assert all(x.startswith('******') for x in result['phone_number'])
        assert result['phone_number'].iloc[0].endswith('5678')
    
    def test_generalize_location(self, anonymizer, sensitive_data):
        """Test location generalization"""
        result = anonymizer.generalize_location(
            sensitive_data.copy(), 
            'latitude', 
            'longitude', 
            precision=1
        )
        
        # Check precision is reduced
        assert result['latitude'].iloc[0] == -1.3
        assert result['longitude'].iloc[0] == 36.8
    
    def test_anonymize_dataset(self, anonymizer, sensitive_data):
        """Test full dataset anonymization"""
        pii_columns = {
            'customer_id': 'hash',
            'phone_number': 'mask',
            'email': 'remove'
        }
        
        result = anonymizer.anonymize_dataset(sensitive_data.copy(), pii_columns)
        
        # Check that transformations were applied
        assert len(result['customer_id'].iloc[0]) == 64  # Hashed
        assert result['phone_number'].iloc[0].startswith('*')  # Masked
        assert 'email' not in result.columns  # Removed


# tests/integration/test_model_pipeline.py
"""
Integration tests for complete model pipeline
"""
import pytest
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from src.models.credit_scoring.model import CreditScoringModel
from src.data_pipeline.validators import DataValidator
from src.data_pipeline.anonymizer import DataAnonymizer


class TestModelPipeline:
    """End-to-end pipeline tests"""
    
    @pytest.fixture
    def sample_training_data(self):
        """Generate synthetic training data"""
        np.random.seed(42)
        n_samples = 1000
        
        return pd.DataFrame({
            'customer_id': [f'C{i:04d}' for i in range(n_samples)],
            'payment_amount': np.random.normal(150, 50, n_samples),
            'payment_date': pd.date_range('2024-01-01', periods=n_samples, freq='D'),
            'last_payment_date': pd.date_range('2024-01-01', periods=n_samples, freq='D'),
            'kwh_used': np.random.normal(10, 3, n_samples),
            'on_time': np.random.binomial(1, 0.7, n_samples),
            'account_created_date': pd.date_range('2023-01-01', periods=n_samples, freq='D'),
            'location_type': np.random.choice(['urban', 'rural'], n_samples),
            'household_size': np.random.randint(1, 8, n_samples),
            'peak_usage': np.random.normal(5, 2, n_samples),
            'weekend_usage': np.random.uniform(0, 1, n_samples),
            'default': np.random.binomial(1, 0.3, n_samples)  # Target variable
        })
    
    def test_complete_pipeline(self, sample_training_data):
        """Test complete pipeline from data validation to prediction"""
        
        # Step 1: Validate data
        validator = DataValidator()
        expected_schema = {
            'customer_id': 'object',
            'payment_amount': 'float64',
            'default': 'int64'
        }
        is_valid, _ = validator.validate_schema(
            sample_training_data[['customer_id', 'payment_amount', 'default']], 
            expected_schema
        )
        assert is_valid
        
        # Step 2: Anonymize sensitive data
        anonymizer = DataAnonymizer()
        anonymized_data = anonymizer.hash_column(sample_training_data.copy(), 'customer_id')
        assert len(anonymized_data) == len(sample_training_data)
        
        # Step 3: Prepare features and train model
        model = CreditScoringModel()
        features_df = model.prepare_features(anonymized_data)
        
        # Select features for training
        feature_cols = [col for col in features_df.columns 
                       if col not in ['customer_id', 'default', 'payment_date', 
                                     'last_payment_date', 'account_created_date']]
        X = features_df[feature_cols]
        y = features_df['default']
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Train model
        metrics = model.train(X_train, y_train, X_test, y_test)
        
        # Assert model performance
        assert metrics['train_auc'] > 0.5
        assert metrics['val_auc'] is not None
        
        # Step 4: Generate predictions
        predictions = model.predict(X_test)
        
        assert len(predictions) == len(X_test)
        assert all(0 <= p <= 1 for p in predictions)
        
    def test_model_saves_and_loads(self, tmp_path, sample_training_data):
        """Test model persistence"""
        
        model = CreditScoringModel()
        features_df = model.prepare_features(sample_training_data)
        
        feature_cols = [col for col in features_df.columns 
                       if col not in ['customer_id', 'default', 'payment_date',
                                     'last_payment_date', 'account_created_date']]
        X = features_df[feature_cols]
        y = features_df['default']
        
        # Train model
        model.train(X, y)
        
        # Save model
        model_path = tmp_path / "test_model.json"
        model.save_model(str(model_path))
        
        # Load model
        new_model = CreditScoringModel()
        new_model.load_model(str(model_path))
        new_model.feature_names = model.feature_names
        
        # Compare predictions
        original_preds = model.predict(X.head(10))
        loaded_preds = new_model.predict(X.head(10))
        
        np.testing.assert_array_almost_equal(original_preds, loaded_preds, decimal=5)


# pytest.ini configuration
"""
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --cov=src
    --cov-report=html
    --cov-report=term-missing
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
"""
